{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stackover_flow_summarizer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq1uMq-VzHIU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c387f92c-1f2f-48f6-a6d6-1f377e80d053"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN3GXbhRzWPF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "9652f3d5-7cb5-40cd-d54a-9a7b32966c33"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 52.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=3930fb2f59b3a8fb20fac35c1404094f24b02559f4f03c3e847ba6e1cb77f4fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE426FdzM4Oa"
      },
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqpB7oZUM8lZ"
      },
      "source": [
        "output_dir = '/content/gdrive/My Drive/stackoverflow/weights/'\n",
        "model = T5ForConditionalGeneration.from_pretrained(output_dir)\n",
        "tokenizer = T5Tokenizer.from_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1u6WAEvN0aE"
      },
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4CEWO4tPrVZ"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht00a8LUN3pZ"
      },
      "source": [
        "import tensorflow as tf\n",
        "train_dataset_fp = tf.keras.utils.get_file('/content/gdrive/My Drive/stackoverflow/summarization_data.csv','summarization_data.csv')\n",
        "batch_size = 8\n",
        "\n",
        "train_dataset = tf.data.experimental.make_csv_dataset(\n",
        "    train_dataset_fp,\n",
        "    batch_size,\n",
        "    select_columns = ['title','body'],\n",
        "    num_epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CW3GGUONUjk"
      },
      "source": [
        "def encode_file(tokenizer,data,max_length,return_tensors=\"pt\"):\n",
        "    examples = []\n",
        "    for sen in data:\n",
        "       tokenized = tokenizer.batch_encode_plus(\n",
        "                [sen], max_length=max_length, truncation=True,padding ='max_length', return_tensors=return_tensors)\n",
        "       examples.append(tokenized)\n",
        "    return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jgrCbvcNXAj"
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoN4Y5D9O7vS"
      },
      "source": [
        "num_train_epochs = 2\n",
        "#model.resize_token_embeddings(len(tokenizer))\n",
        "weight_decay =0.0\n",
        "learning_rate = 1e-4 \n",
        "adam_epsilon = 1e-8\n",
        "warmup_steps = 0\n",
        "t_total = (607276 // 8 ) * float(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WojEhkmhPlXg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bd12be5b-cf7b-442d-8999-5644cdcd6535"
      },
      "source": [
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "optimizer.load_state_dict(torch.load(os.path.join(output_dir, 'optimizer.pt')))\n",
        "scheduler.load_state_dict(torch.load(os.path.join(output_dir, 'scheduler.pt')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UScR8NWNTunR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSUZXgKRPtsQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23883cf5-50bf-4c70-d46c-d4ac850023c6"
      },
      "source": [
        "epochs = 5\n",
        "for epoc in range(epochs):\n",
        "  t0 = time.time()\n",
        "  print(\"\")\n",
        "  print('======== Epoch {} ========'.format(epoc+1))\n",
        "  model.train()\n",
        "  total_train_loss = 0\n",
        "  for i,batch in enumerate(train_dataset):\n",
        "    title = []\n",
        "    body = []\n",
        "    for item in batch['title'].numpy():\n",
        "      title.append(item.decode('utf-8'))\n",
        "    for item in batch['body'].numpy():\n",
        "      body.append(item.decode('utf-8')) \n",
        "    encoded_title = encode_file(tokenizer,title,120)\n",
        "    encoded_body = encode_file(tokenizer,body,512)\n",
        "    ids = torch.stack([x[\"input_ids\"] for x in encoded_body]).squeeze()# BS x SL\n",
        "    mask = torch.stack([x[\"attention_mask\"] for x in encoded_body]).squeeze() # BS x SL\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    y = torch.stack([x[\"input_ids\"] for x in encoded_title]).squeeze()\n",
        "    y = y.to(device, dtype = torch.long)\n",
        "    y_ids = y[:, :-1].contiguous()\n",
        "    lm_labels = y[:, 1:].clone().detach()\n",
        "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "    ids = ids.to(device, dtype = torch.long)\n",
        "    mask = mask.to(device, dtype = torch.long)\n",
        "    model.zero_grad()\n",
        "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "    loss = outputs[0]\n",
        "    total_train_loss += loss.item()\n",
        "    \n",
        "    if i%500 == 0:\n",
        "      print(\"batch :\" + str(i)+ \"Training Loss: \" +str(total_train_loss))\n",
        "    if i % 20000 == 0:\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
        "        torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "  training_time = format_time(time.time() - t0)\n",
        "  avg_train_loss = total_train_loss / 1050994\n",
        "  print(\"traing loss epoch\"+str(epoc+1)+\":\"+str(avg_train_loss))\n",
        "  print(\"time : {}\".format(training_time))\n",
        "  \n",
        "  model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "  model_to_save.save_pretrained(output_dir)\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "  torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
        "  torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 ========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py:1128: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch :0Training Loss: 5.187439918518066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch :500Training Loss: 1533.8602048158646\n",
            "batch :1000Training Loss: 2913.984565615654\n",
            "batch :1500Training Loss: 4241.05873811245\n",
            "batch :2000Training Loss: 5567.6087691783905\n",
            "batch :2500Training Loss: 6862.107839345932\n",
            "batch :3000Training Loss: 8147.294431686401\n",
            "batch :3500Training Loss: 9435.012337207794\n",
            "batch :4000Training Loss: 10705.89479315281\n",
            "batch :4500Training Loss: 11972.306923747063\n",
            "batch :5000Training Loss: 13214.92782485485\n",
            "batch :5500Training Loss: 14471.89020550251\n",
            "batch :6000Training Loss: 15712.807140946388\n",
            "batch :6500Training Loss: 16954.91726064682\n",
            "batch :7000Training Loss: 18192.134781599045\n",
            "batch :7500Training Loss: 19419.502864599228\n",
            "batch :8000Training Loss: 20651.13777422905\n",
            "batch :8500Training Loss: 21884.291428804398\n",
            "batch :9000Training Loss: 23108.809393525124\n",
            "batch :9500Training Loss: 24329.61497926712\n",
            "batch :10000Training Loss: 25555.460893154144\n",
            "batch :10500Training Loss: 26775.082923173904\n",
            "batch :11000Training Loss: 28004.104807019234\n",
            "batch :11500Training Loss: 29219.49579179287\n",
            "batch :12000Training Loss: 30435.404669880867\n",
            "batch :12500Training Loss: 31629.898299574852\n",
            "batch :13000Training Loss: 32819.980075240135\n",
            "batch :13500Training Loss: 34031.97654080391\n",
            "batch :14000Training Loss: 35227.57068789005\n",
            "batch :14500Training Loss: 36431.06394505501\n",
            "batch :15000Training Loss: 37635.766367316246\n",
            "batch :15500Training Loss: 38836.851252794266\n",
            "batch :16000Training Loss: 40025.55380117893\n",
            "batch :16500Training Loss: 41220.4425727129\n",
            "batch :17000Training Loss: 42425.14616191387\n",
            "batch :17500Training Loss: 43620.983996629715\n",
            "batch :18000Training Loss: 44804.54091382027\n",
            "batch :18500Training Loss: 45999.68722176552\n",
            "batch :19000Training Loss: 47194.4964556694\n",
            "batch :19500Training Loss: 48369.5731164217\n",
            "batch :20000Training Loss: 49556.991611242294\n",
            "batch :20500Training Loss: 50735.54105198383\n",
            "batch :21000Training Loss: 51943.17964351177\n",
            "batch :21500Training Loss: 53117.16375553608\n",
            "batch :22000Training Loss: 54304.54763066769\n",
            "batch :22500Training Loss: 55476.52041876316\n",
            "batch :23000Training Loss: 56662.26193225384\n",
            "batch :23500Training Loss: 57829.687791347504\n",
            "batch :24000Training Loss: 58998.804092526436\n",
            "batch :24500Training Loss: 60178.59510302544\n",
            "batch :25000Training Loss: 61356.504736185074\n",
            "batch :25500Training Loss: 62532.872569441795\n",
            "batch :26000Training Loss: 63710.67120921612\n",
            "batch :26500Training Loss: 64891.2272490263\n",
            "batch :27000Training Loss: 66065.4501260519\n",
            "batch :27500Training Loss: 67221.40329229832\n",
            "batch :28000Training Loss: 68372.48994934559\n",
            "batch :28500Training Loss: 69523.4654752016\n",
            "batch :29000Training Loss: 70685.56783223152\n",
            "batch :29500Training Loss: 71847.14852237701\n",
            "batch :30000Training Loss: 73010.30054986477\n",
            "batch :30500Training Loss: 74170.25615549088\n",
            "batch :31000Training Loss: 75339.36187493801\n",
            "batch :31500Training Loss: 76495.04712593555\n",
            "batch :32000Training Loss: 77653.92184484005\n",
            "batch :32500Training Loss: 78796.31194388866\n",
            "batch :33000Training Loss: 79950.36642026901\n",
            "batch :33500Training Loss: 81114.62912762165\n",
            "batch :34000Training Loss: 82267.42897284031\n",
            "batch :34500Training Loss: 83439.25383460522\n",
            "batch :35000Training Loss: 84585.86737763882\n",
            "batch :35500Training Loss: 85732.46363949776\n",
            "batch :36000Training Loss: 86867.54054749012\n",
            "batch :36500Training Loss: 88034.63357794285\n",
            "batch :37000Training Loss: 89186.8134740591\n",
            "batch :37500Training Loss: 90357.479342103\n",
            "batch :38000Training Loss: 91512.14319431782\n",
            "batch :38500Training Loss: 92657.4631755352\n",
            "batch :39000Training Loss: 93818.8465641737\n",
            "batch :39500Training Loss: 94962.34803152084\n",
            "batch :40000Training Loss: 96110.96727001667\n",
            "batch :40500Training Loss: 97252.264041543\n",
            "batch :41000Training Loss: 98405.23434817791\n",
            "batch :41500Training Loss: 99531.21707940102\n",
            "batch :42000Training Loss: 100677.03373861313\n",
            "batch :42500Training Loss: 101819.76215875149\n",
            "batch :43000Training Loss: 102972.89223957062\n",
            "batch :43500Training Loss: 104115.19682133198\n",
            "batch :44000Training Loss: 105255.29306745529\n",
            "batch :44500Training Loss: 106404.71137464046\n",
            "batch :45000Training Loss: 107535.74818944931\n",
            "batch :45500Training Loss: 108682.30648446083\n",
            "batch :46000Training Loss: 109816.97091901302\n",
            "batch :46500Training Loss: 110963.21912765503\n",
            "batch :47000Training Loss: 112093.79096376896\n",
            "batch :47500Training Loss: 113244.26128590107\n",
            "batch :48000Training Loss: 114376.60518753529\n",
            "batch :48500Training Loss: 115505.03988957405\n",
            "batch :49000Training Loss: 116627.21663439274\n",
            "batch :49500Training Loss: 117751.39862203598\n",
            "batch :50000Training Loss: 118890.65613305569\n",
            "batch :50500Training Loss: 120011.92802345753\n",
            "batch :51000Training Loss: 121137.61100232601\n",
            "batch :51500Training Loss: 122265.11652863026\n",
            "batch :52000Training Loss: 123394.66497516632\n",
            "batch :52500Training Loss: 124521.91708803177\n",
            "batch :53000Training Loss: 125646.17491734028\n",
            "batch :53500Training Loss: 126776.2323050499\n",
            "batch :54000Training Loss: 127909.20876085758\n",
            "batch :54500Training Loss: 129041.36628675461\n",
            "batch :55000Training Loss: 130174.37500882149\n",
            "batch :55500Training Loss: 131297.28485298157\n",
            "batch :56000Training Loss: 132419.47014200687\n",
            "batch :56500Training Loss: 133560.08324480057\n",
            "batch :57000Training Loss: 134676.39244651794\n",
            "batch :57500Training Loss: 135829.9438790083\n",
            "batch :58000Training Loss: 136960.22648465633\n",
            "batch :58500Training Loss: 138088.7897925377\n",
            "batch :59000Training Loss: 139214.63103568554\n",
            "batch :59500Training Loss: 140335.62728738785\n",
            "batch :60000Training Loss: 141469.50877928734\n",
            "batch :60500Training Loss: 142622.82395899296\n",
            "batch :61000Training Loss: 143756.21073293686\n",
            "batch :61500Training Loss: 144891.27421581745\n",
            "batch :62000Training Loss: 146029.81848490238\n",
            "batch :62500Training Loss: 147150.98524206877\n",
            "batch :63000Training Loss: 148277.30845087767\n",
            "batch :63500Training Loss: 149401.9212784171\n",
            "batch :64000Training Loss: 150525.33853012323\n",
            "batch :64500Training Loss: 151667.47218984365\n",
            "batch :65000Training Loss: 152785.113933146\n",
            "batch :65500Training Loss: 153919.3355782628\n",
            "batch :66000Training Loss: 155041.61673349142\n",
            "batch :66500Training Loss: 156158.87330907583\n",
            "batch :67000Training Loss: 157272.51671987772\n",
            "batch :67500Training Loss: 158403.2102677226\n",
            "batch :68000Training Loss: 159532.8119468093\n",
            "batch :68500Training Loss: 160667.42777949572\n",
            "batch :69000Training Loss: 161776.64282304049\n",
            "batch :69500Training Loss: 162899.17922478914\n",
            "batch :70000Training Loss: 164021.20871204138\n",
            "batch :70500Training Loss: 165151.2928672433\n",
            "batch :71000Training Loss: 166276.4136107564\n",
            "batch :71500Training Loss: 167408.2330107093\n",
            "batch :72000Training Loss: 168529.54347640276\n",
            "batch :72500Training Loss: 169662.4034500718\n",
            "batch :73000Training Loss: 170798.32598024607\n",
            "batch :73500Training Loss: 171927.56185096502\n",
            "batch :74000Training Loss: 173048.61924785376\n",
            "batch :74500Training Loss: 174176.8378520608\n",
            "batch :75000Training Loss: 175307.40482264757\n",
            "batch :75500Training Loss: 176440.14612728357\n",
            "traing loss epoch1:0.16875917724605813\n",
            "time : 5:39:27\n",
            "\n",
            "======== Epoch 2 ========\n",
            "batch :0Training Loss: 2.0884130001068115\n",
            "batch :500Training Loss: 1054.545999288559\n",
            "batch :1000Training Loss: 2111.572109222412\n",
            "batch :1500Training Loss: 3173.766675710678\n",
            "batch :2000Training Loss: 4244.841140389442\n",
            "batch :2500Training Loss: 5290.169262051582\n",
            "batch :3000Training Loss: 6356.183490991592\n",
            "batch :3500Training Loss: 7422.053071141243\n",
            "batch :4000Training Loss: 8483.793473005295\n",
            "batch :4500Training Loss: 9540.30540663004\n",
            "batch :5000Training Loss: 10587.489384114742\n",
            "batch :5500Training Loss: 11655.563430130482\n",
            "batch :6000Training Loss: 12727.694156110287\n",
            "batch :6500Training Loss: 13788.499285876751\n",
            "batch :7000Training Loss: 14841.302302539349\n",
            "batch :7500Training Loss: 15895.145717322826\n",
            "batch :8000Training Loss: 16951.944001972675\n",
            "batch :8500Training Loss: 18022.90895074606\n",
            "batch :9000Training Loss: 19105.42885953188\n",
            "batch :9500Training Loss: 20180.036859214306\n",
            "batch :10000Training Loss: 21243.85587567091\n",
            "batch :10500Training Loss: 22311.967160642147\n",
            "batch :11000Training Loss: 23377.695062935352\n",
            "batch :11500Training Loss: 24446.99517840147\n",
            "batch :12000Training Loss: 25510.114623963833\n",
            "batch :12500Training Loss: 26576.494392216206\n",
            "batch :13000Training Loss: 27638.933498322964\n",
            "batch :13500Training Loss: 28708.419485390186\n",
            "batch :14000Training Loss: 29768.549547731876\n",
            "batch :14500Training Loss: 30838.070146143436\n",
            "batch :15000Training Loss: 31900.78864878416\n",
            "batch :15500Training Loss: 32959.076195299625\n",
            "batch :16000Training Loss: 34041.84297555685\n",
            "batch :16500Training Loss: 35105.646945774555\n",
            "batch :17000Training Loss: 36176.369974792\n",
            "batch :17500Training Loss: 37226.34206777811\n",
            "batch :18000Training Loss: 38292.42419975996\n",
            "batch :18500Training Loss: 39355.20877081156\n",
            "batch :19000Training Loss: 40425.70573228598\n",
            "batch :19500Training Loss: 41495.59939724207\n",
            "batch :20000Training Loss: 42550.7861045599\n",
            "batch :20500Training Loss: 43626.21857881546\n",
            "batch :21000Training Loss: 44700.500344097614\n",
            "batch :21500Training Loss: 45771.19409519434\n",
            "batch :22000Training Loss: 46835.200367987156\n",
            "batch :22500Training Loss: 47914.15386861563\n",
            "batch :23000Training Loss: 48990.39951783419\n",
            "batch :23500Training Loss: 50039.259185016155\n",
            "batch :24000Training Loss: 51107.253427028656\n",
            "batch :24500Training Loss: 52157.20376884937\n",
            "batch :25000Training Loss: 53229.48545897007\n",
            "batch :25500Training Loss: 54305.47000360489\n",
            "batch :26000Training Loss: 55393.08581650257\n",
            "batch :26500Training Loss: 56462.95136141777\n",
            "batch :27000Training Loss: 57530.57059466839\n",
            "batch :27500Training Loss: 58585.76003694534\n",
            "batch :28000Training Loss: 59655.31869006157\n",
            "batch :28500Training Loss: 60724.025842905045\n",
            "batch :29000Training Loss: 61782.47088420391\n",
            "batch :29500Training Loss: 62841.12786304951\n",
            "batch :30000Training Loss: 63907.17182481289\n",
            "batch :30500Training Loss: 64981.06079041958\n",
            "batch :31000Training Loss: 66048.68463051319\n",
            "batch :31500Training Loss: 67106.34660756588\n",
            "batch :32000Training Loss: 68168.94643491507\n",
            "batch :32500Training Loss: 69236.22871488333\n",
            "batch :33000Training Loss: 70306.0587721467\n",
            "batch :33500Training Loss: 71384.24939745665\n",
            "batch :34000Training Loss: 72447.60722237825\n",
            "batch :34500Training Loss: 73518.80955606699\n",
            "batch :35000Training Loss: 74586.89824074507\n",
            "batch :35500Training Loss: 75636.70344334841\n",
            "batch :36000Training Loss: 76688.45808070898\n",
            "batch :36500Training Loss: 77754.82352298498\n",
            "batch :37000Training Loss: 78829.83148437738\n",
            "batch :37500Training Loss: 79906.390896976\n",
            "batch :38000Training Loss: 80982.43480962515\n",
            "batch :38500Training Loss: 82053.95531851053\n",
            "batch :39000Training Loss: 83125.93283754587\n",
            "batch :39500Training Loss: 84189.72870475054\n",
            "batch :40000Training Loss: 85255.82709187269\n",
            "batch :40500Training Loss: 86334.50938028097\n",
            "batch :41000Training Loss: 87400.5204834342\n",
            "batch :41500Training Loss: 88470.60641896725\n",
            "batch :42000Training Loss: 89534.51471734047\n",
            "batch :42500Training Loss: 90609.89030730724\n",
            "batch :43000Training Loss: 91675.90363729\n",
            "batch :43500Training Loss: 92740.65367221832\n",
            "batch :44000Training Loss: 93811.79456698895\n",
            "batch :44500Training Loss: 94871.762316823\n",
            "batch :45000Training Loss: 95955.77335965633\n",
            "batch :45500Training Loss: 97025.03832161427\n",
            "batch :46000Training Loss: 98096.76096004248\n",
            "batch :46500Training Loss: 99156.34023243189\n",
            "batch :47000Training Loss: 100224.36468726397\n",
            "batch :47500Training Loss: 101292.025560081\n",
            "batch :48000Training Loss: 102365.27927881479\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o0cC_VmLJWC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46c3c05b-de8d-4352-ee15-468203807840"
      },
      "source": [
        "epochs = 5\n",
        "for epoc in range(epochs):\n",
        "  t0 = time.time()\n",
        "  print(\"\")\n",
        "  print('======== Epoch {} ========'.format(epoc+1))\n",
        "  model.train()\n",
        "  total_train_loss = 0\n",
        "  for i,batch in enumerate(train_dataset):\n",
        "    title = []\n",
        "    body = []\n",
        "    for item in batch['title'].numpy():\n",
        "      title.append(item.decode('utf-8'))\n",
        "    for item in batch['body'].numpy():\n",
        "      body.append(item.decode('utf-8')) \n",
        "    encoded_title = encode_file(tokenizer,title,120)\n",
        "    encoded_body = encode_file(tokenizer,body,512)\n",
        "    ids = torch.stack([x[\"input_ids\"] for x in encoded_body]).squeeze()# BS x SL\n",
        "    mask = torch.stack([x[\"attention_mask\"] for x in encoded_body]).squeeze() # BS x SL\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    y = torch.stack([x[\"input_ids\"] for x in encoded_title]).squeeze()\n",
        "    y = y.to(device, dtype = torch.long)\n",
        "    y_ids = y[:, :-1].contiguous()\n",
        "    lm_labels = y[:, 1:].clone().detach()\n",
        "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "    ids = ids.to(device, dtype = torch.long)\n",
        "    mask = mask.to(device, dtype = torch.long)\n",
        "    model.zero_grad()\n",
        "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "    loss = outputs[0]\n",
        "    total_train_loss += loss.item()\n",
        "    \n",
        "    if i%500 == 0:\n",
        "      print(\"batch :\" + str(i)+ \"Training Loss: \" +str(total_train_loss))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "  training_time = format_time(time.time() - t0)\n",
        "  avg_train_loss = total_train_loss / 1050994\n",
        "  print(\"traing loss epoch\"+str(epoc+1)+\":\"+str(avg_train_loss))\n",
        "  print(\"time : {}\".format(training_time))\n",
        "  \n",
        "  model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "  model_to_save.save_pretrained(output_dir)\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "  torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
        "  torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 ========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py:1128: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch :0Training Loss: 5.295326232910156\n",
            "batch :500Training Loss: 1528.4088693857193\n",
            "batch :1000Training Loss: 2921.5784620046616\n",
            "batch :1500Training Loss: 4270.449675321579\n",
            "batch :2000Training Loss: 5579.342727184296\n",
            "batch :2500Training Loss: 6871.840752840042\n",
            "batch :3000Training Loss: 8162.124095320702\n",
            "batch :3500Training Loss: 9444.195762991905\n",
            "batch :4000Training Loss: 10714.794030189514\n",
            "batch :4500Training Loss: 11988.838445544243\n",
            "batch :5000Training Loss: 13228.565332174301\n",
            "batch :5500Training Loss: 14484.114659190178\n",
            "batch :6000Training Loss: 15722.980154633522\n",
            "batch :6500Training Loss: 16950.769930958748\n",
            "batch :7000Training Loss: 18169.60313117504\n",
            "batch :7500Training Loss: 19406.309152126312\n",
            "batch :8000Training Loss: 20632.255396723747\n",
            "batch :8500Training Loss: 21857.136608600616\n",
            "batch :9000Training Loss: 23089.353440523148\n",
            "batch :9500Training Loss: 24307.540618538857\n",
            "batch :10000Training Loss: 25543.613577723503\n",
            "batch :10500Training Loss: 26782.20229613781\n",
            "batch :11000Training Loss: 28008.69323658943\n",
            "batch :11500Training Loss: 29233.505742430687\n",
            "batch :12000Training Loss: 30442.533299565315\n",
            "batch :12500Training Loss: 31635.570271134377\n",
            "batch :13000Training Loss: 32838.122708678246\n",
            "batch :13500Training Loss: 34046.63919353485\n",
            "batch :14000Training Loss: 35250.26058149338\n",
            "batch :14500Training Loss: 36451.31989121437\n",
            "batch :15000Training Loss: 37654.19610762596\n",
            "batch :15500Training Loss: 38868.079089164734\n",
            "batch :16000Training Loss: 40061.05838012695\n",
            "batch :16500Training Loss: 41252.41493880749\n",
            "batch :17000Training Loss: 42457.11193108559\n",
            "batch :17500Training Loss: 43661.274193525314\n",
            "batch :18000Training Loss: 44839.502237677574\n",
            "batch :18500Training Loss: 46024.248499155045\n",
            "batch :19000Training Loss: 47210.75162613392\n",
            "batch :19500Training Loss: 48393.41351521015\n",
            "batch :20000Training Loss: 49572.877383589745\n",
            "batch :20500Training Loss: 50757.85440027714\n",
            "batch :21000Training Loss: 51953.285815835\n",
            "batch :21500Training Loss: 53117.65143716335\n",
            "batch :22000Training Loss: 54297.831923246384\n",
            "batch :22500Training Loss: 55463.48853266239\n",
            "batch :23000Training Loss: 56641.41359269619\n",
            "batch :23500Training Loss: 57832.25981283188\n",
            "batch :24000Training Loss: 59016.24929344654\n",
            "batch :24500Training Loss: 60194.730170726776\n",
            "batch :25000Training Loss: 61369.71978843212\n",
            "batch :25500Training Loss: 62558.728504896164\n",
            "batch :26000Training Loss: 63729.26709628105\n",
            "batch :26500Training Loss: 64906.90991663933\n",
            "batch :27000Training Loss: 66070.75204527378\n",
            "batch :27500Training Loss: 67231.26693952084\n",
            "batch :28000Training Loss: 68391.31258785725\n",
            "batch :28500Training Loss: 69561.71189963818\n",
            "batch :29000Training Loss: 70710.2733360529\n",
            "batch :29500Training Loss: 71879.29048085213\n",
            "batch :30000Training Loss: 73048.89699065685\n",
            "batch :30500Training Loss: 74210.00165975094\n",
            "batch :31000Training Loss: 75375.18861877918\n",
            "batch :31500Training Loss: 76541.55580663681\n",
            "batch :32000Training Loss: 77685.61032307148\n",
            "batch :32500Training Loss: 78849.45375525951\n",
            "batch :33000Training Loss: 79998.49112093449\n",
            "batch :33500Training Loss: 81147.3877196312\n",
            "batch :34000Training Loss: 82313.7209070921\n",
            "batch :34500Training Loss: 83476.18941831589\n",
            "batch :35000Training Loss: 84623.74223053455\n",
            "batch :35500Training Loss: 85769.82496225834\n",
            "batch :36000Training Loss: 86915.48691856861\n",
            "batch :36500Training Loss: 88064.7458319664\n",
            "batch :37000Training Loss: 89221.29581069946\n",
            "batch :37500Training Loss: 90369.44279718399\n",
            "batch :38000Training Loss: 91526.44842863083\n",
            "batch :38500Training Loss: 92686.8046144247\n",
            "batch :39000Training Loss: 93842.93312394619\n",
            "batch :39500Training Loss: 95000.92177796364\n",
            "batch :40000Training Loss: 96148.0604082346\n",
            "batch :40500Training Loss: 97310.70447921753\n",
            "batch :41000Training Loss: 98450.25970733166\n",
            "batch :41500Training Loss: 99606.12801086903\n",
            "batch :42000Training Loss: 100738.09084391594\n",
            "batch :42500Training Loss: 101888.70529043674\n",
            "batch :43000Training Loss: 103023.97305369377\n",
            "batch :43500Training Loss: 104166.62724888325\n",
            "batch :44000Training Loss: 105299.5535120964\n",
            "batch :44500Training Loss: 106437.1433981657\n",
            "batch :45000Training Loss: 107574.93694710732\n",
            "batch :45500Training Loss: 108704.38240647316\n",
            "batch :46000Training Loss: 109857.95173883438\n",
            "batch :46500Training Loss: 111003.43824267387\n",
            "batch :47000Training Loss: 112134.61467766762\n",
            "batch :47500Training Loss: 113264.93026447296\n",
            "batch :48000Training Loss: 114399.39724171162\n",
            "batch :48500Training Loss: 115531.54978442192\n",
            "batch :49000Training Loss: 116660.15533304214\n",
            "batch :49500Training Loss: 117785.66319286823\n",
            "batch :50000Training Loss: 118912.2634125948\n",
            "batch :50500Training Loss: 120046.72223007679\n",
            "batch :51000Training Loss: 121185.33980083466\n",
            "batch :51500Training Loss: 122307.43510645628\n",
            "batch :52000Training Loss: 123434.37645083666\n",
            "batch :52500Training Loss: 124559.1824195981\n",
            "batch :53000Training Loss: 125695.07041341066\n",
            "batch :53500Training Loss: 126828.12188142538\n",
            "batch :54000Training Loss: 127952.36395704746\n",
            "batch :54500Training Loss: 129089.60641682148\n",
            "batch :55000Training Loss: 130212.70617604256\n",
            "batch :55500Training Loss: 131342.43952834606\n",
            "batch :56000Training Loss: 132477.36182582378\n",
            "batch :56500Training Loss: 133614.39426517487\n",
            "batch :57000Training Loss: 134745.76246201992\n",
            "batch :57500Training Loss: 135875.46115481853\n",
            "batch :58000Training Loss: 137002.10395359993\n",
            "batch :58500Training Loss: 138125.73307085037\n",
            "batch :59000Training Loss: 139264.7729370594\n",
            "batch :59500Training Loss: 140392.08175194263\n",
            "batch :60000Training Loss: 141530.51126754284\n",
            "batch :60500Training Loss: 142663.72971129417\n",
            "batch :61000Training Loss: 143800.08764255047\n",
            "batch :61500Training Loss: 144922.97480106354\n",
            "batch :62000Training Loss: 146052.39501190186\n",
            "batch :62500Training Loss: 147184.8401979208\n",
            "batch :63000Training Loss: 148313.06010127068\n",
            "batch :63500Training Loss: 149452.4975503683\n",
            "batch :64000Training Loss: 150574.24535405636\n",
            "batch :64500Training Loss: 151697.98535954952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0ivFAZHqNYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2340fa-09f8-49f1-eeea-8b4f9f5be768"
      },
      "source": [
        "epochs = 5\n",
        "for epoc in range(epochs):\n",
        "  t0 = time.time()\n",
        "  print(\"\")\n",
        "  print('======== Epoch {} ========'.format(epoc+1))\n",
        "  model.train()\n",
        "  total_train_loss = 0\n",
        "  for i,batch in enumerate(train_dataset):\n",
        "    title = []\n",
        "    body = []\n",
        "    for item in batch['title'].numpy():\n",
        "      title.append(item.decode('utf-8'))\n",
        "    for item in batch['body'].numpy():\n",
        "      body.append(item.decode('utf-8')) \n",
        "    encoded_title = encode_file(tokenizer,title,120)\n",
        "    encoded_body = encode_file(tokenizer,body,512)\n",
        "    ids = torch.stack([x[\"input_ids\"] for x in encoded_body]).squeeze()# BS x SL\n",
        "    mask = torch.stack([x[\"attention_mask\"] for x in encoded_body]).squeeze() # BS x SL\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    y = torch.stack([x[\"input_ids\"] for x in encoded_title]).squeeze()\n",
        "    y = y.to(device, dtype = torch.long)\n",
        "    y_ids = y[:, :-1].contiguous()\n",
        "    lm_labels = y[:, 1:].clone().detach()\n",
        "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "    ids = ids.to(device, dtype = torch.long)\n",
        "    mask = mask.to(device, dtype = torch.long)\n",
        "    model.zero_grad()\n",
        "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "    loss = outputs[0]\n",
        "    total_train_loss += loss.item()\n",
        "    \n",
        "    if i%500 == 0:\n",
        "      print(\"batch :\" + str(i)+ \"Training Loss: \" +str(total_train_loss))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "  training_time = format_time(time.time() - t0)\n",
        "  avg_train_loss = total_train_loss / 1050994\n",
        "  print(\"traing loss epoch\"+str(epoc+1)+\":\"+str(avg_train_loss))\n",
        "  print(\"time : {}\".format(training_time))\n",
        "  \n",
        "  model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "  model_to_save.save_pretrained(output_dir)\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "  torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
        "  torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 ========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py:1128: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch :0Training Loss: 2.044417142868042\n",
            "batch :500Training Loss: 1007.9309905767441\n",
            "batch :1000Training Loss: 2018.7188791036606\n",
            "batch :1500Training Loss: 3046.364337205887\n",
            "batch :2000Training Loss: 4070.3735456466675\n",
            "batch :2500Training Loss: 5108.851502358913\n",
            "batch :3000Training Loss: 6130.290075838566\n",
            "batch :3500Training Loss: 7154.580143868923\n",
            "batch :4000Training Loss: 8181.764933288097\n",
            "batch :4500Training Loss: 9219.019903123379\n",
            "batch :5000Training Loss: 10246.710900008678\n",
            "batch :5500Training Loss: 11280.93491858244\n",
            "batch :6000Training Loss: 12296.160244166851\n",
            "batch :6500Training Loss: 13318.473236382008\n",
            "batch :7000Training Loss: 14340.052779138088\n",
            "batch :7500Training Loss: 15373.599617660046\n",
            "batch :8000Training Loss: 16413.39070022106\n",
            "batch :8500Training Loss: 17435.94557440281\n",
            "batch :9000Training Loss: 18476.59170806408\n",
            "batch :9500Training Loss: 19514.4249856472\n",
            "batch :10000Training Loss: 20545.569168686867\n",
            "batch :10500Training Loss: 21600.828709363937\n",
            "batch :11000Training Loss: 22636.854675412178\n",
            "batch :11500Training Loss: 23687.980956554413\n",
            "batch :12000Training Loss: 24732.612607359886\n",
            "batch :12500Training Loss: 25768.688111782074\n",
            "batch :13000Training Loss: 26799.490826129913\n",
            "batch :13500Training Loss: 27829.353852272034\n",
            "batch :14000Training Loss: 28856.505412817\n",
            "batch :14500Training Loss: 29887.120717048645\n",
            "batch :15000Training Loss: 30926.873920202255\n",
            "batch :15500Training Loss: 31961.66393005848\n",
            "batch :16000Training Loss: 32992.20938575268\n",
            "batch :16500Training Loss: 34048.25420832634\n",
            "batch :17000Training Loss: 35090.44887149334\n",
            "batch :17500Training Loss: 36134.78964078426\n",
            "batch :18000Training Loss: 37170.36012327671\n",
            "batch :18500Training Loss: 38206.11517345905\n",
            "batch :19000Training Loss: 39246.975697636604\n",
            "batch :19500Training Loss: 40287.43131673336\n",
            "batch :20000Training Loss: 41329.26128637791\n",
            "batch :20500Training Loss: 42375.25821900368\n",
            "batch :21000Training Loss: 43417.60433387756\n",
            "batch :21500Training Loss: 44458.314464211464\n",
            "batch :22000Training Loss: 45506.45733964443\n",
            "batch :22500Training Loss: 46553.2752058506\n",
            "batch :23000Training Loss: 47585.05833095312\n",
            "batch :23500Training Loss: 48628.47121530771\n",
            "batch :24000Training Loss: 49683.09719198942\n",
            "batch :24500Training Loss: 50726.56251436472\n",
            "batch :25000Training Loss: 51785.78813570738\n",
            "batch :25500Training Loss: 52831.56069427729\n",
            "batch :26000Training Loss: 53884.317870914936\n",
            "batch :26500Training Loss: 54937.12945383787\n",
            "batch :27000Training Loss: 55988.15007811785\n",
            "batch :27500Training Loss: 57046.6974273324\n",
            "batch :28000Training Loss: 58086.47746050358\n",
            "batch :28500Training Loss: 59116.68772614002\n",
            "batch :29000Training Loss: 60157.6266387701\n",
            "batch :29500Training Loss: 61198.344656050205\n",
            "batch :30000Training Loss: 62236.869279801846\n",
            "batch :30500Training Loss: 63283.22249656916\n",
            "batch :31000Training Loss: 64334.936414182186\n",
            "batch :31500Training Loss: 65376.5802847743\n",
            "batch :32000Training Loss: 66420.30085963011\n",
            "batch :32500Training Loss: 67459.0754878521\n",
            "batch :33000Training Loss: 68500.06856203079\n",
            "batch :33500Training Loss: 69560.96842515469\n",
            "batch :34000Training Loss: 70612.92595589161\n",
            "batch :34500Training Loss: 71664.15619897842\n",
            "batch :35000Training Loss: 72713.37604188919\n",
            "batch :35500Training Loss: 73759.37928891182\n",
            "batch :36000Training Loss: 74814.84911954403\n",
            "batch :36500Training Loss: 75856.41332793236\n",
            "batch :37000Training Loss: 76918.01394891739\n",
            "batch :37500Training Loss: 77958.57321023941\n",
            "batch :38000Training Loss: 79036.18574047089\n",
            "batch :38500Training Loss: 80097.57345354557\n",
            "batch :39000Training Loss: 81145.74106180668\n",
            "batch :39500Training Loss: 82206.6318808794\n",
            "batch :40000Training Loss: 83270.47344374657\n",
            "batch :40500Training Loss: 84328.51338016987\n",
            "batch :41000Training Loss: 85396.93917608261\n",
            "batch :41500Training Loss: 86454.35998356342\n",
            "batch :42000Training Loss: 87514.75064456463\n",
            "batch :42500Training Loss: 88583.683208704\n",
            "batch :43000Training Loss: 89647.35471308231\n",
            "batch :43500Training Loss: 90714.13232684135\n",
            "batch :44000Training Loss: 91793.36432397366\n",
            "batch :44500Training Loss: 92854.28656291962\n",
            "batch :45000Training Loss: 93931.48967778683\n",
            "batch :45500Training Loss: 94994.37003695965\n",
            "batch :46000Training Loss: 96048.8616155386\n",
            "batch :46500Training Loss: 97117.25930726528\n",
            "batch :47000Training Loss: 98175.26045942307\n",
            "batch :47500Training Loss: 99237.23235106468\n",
            "batch :48000Training Loss: 100307.44278746843\n",
            "batch :48500Training Loss: 101366.68088418245\n",
            "batch :49000Training Loss: 102427.00098603964\n",
            "batch :49500Training Loss: 103481.27178639174\n",
            "batch :50000Training Loss: 104565.03521949053\n",
            "batch :50500Training Loss: 105634.50000458956\n",
            "batch :51000Training Loss: 106701.44338232279\n",
            "batch :51500Training Loss: 107761.11092346907\n",
            "batch :52000Training Loss: 108835.08923435211\n",
            "batch :52500Training Loss: 109901.65413248539\n",
            "batch :53000Training Loss: 110955.71749961376\n",
            "batch :53500Training Loss: 112007.69170820713\n",
            "batch :54000Training Loss: 113087.54721701145\n",
            "batch :54500Training Loss: 114157.23367726803\n",
            "batch :55000Training Loss: 115228.05312883854\n",
            "batch :55500Training Loss: 116289.19719529152\n",
            "batch :56000Training Loss: 117360.43190729618\n",
            "batch :56500Training Loss: 118441.19017291069\n",
            "batch :57000Training Loss: 119522.59193396568\n",
            "batch :57500Training Loss: 120600.1268593669\n",
            "batch :58000Training Loss: 121672.3432059884\n",
            "batch :58500Training Loss: 122744.97632795572\n",
            "batch :59000Training Loss: 123813.68337970972\n",
            "batch :59500Training Loss: 124888.14859265089\n",
            "batch :60000Training Loss: 125967.19368356466\n",
            "batch :60500Training Loss: 127047.64828556776\n",
            "batch :61000Training Loss: 128131.60291570425\n",
            "batch :61500Training Loss: 129217.98602455854\n",
            "batch :62000Training Loss: 130306.72733587027\n",
            "batch :62500Training Loss: 131392.62441831827\n",
            "batch :63000Training Loss: 132470.29526430368\n",
            "batch :63500Training Loss: 133560.81623071432\n",
            "batch :64000Training Loss: 134644.44754880667\n",
            "batch :64500Training Loss: 135715.10086089373\n",
            "batch :65000Training Loss: 136793.09409230947\n",
            "batch :65500Training Loss: 137874.52910465002\n",
            "batch :66000Training Loss: 138961.28748637438\n",
            "batch :66500Training Loss: 140048.9505674839\n",
            "batch :67000Training Loss: 141112.82322454453\n",
            "batch :67500Training Loss: 142190.10129261017\n",
            "batch :68000Training Loss: 143272.79948163033\n",
            "batch :68500Training Loss: 144354.89560973644\n",
            "batch :69000Training Loss: 145454.99168479443\n",
            "batch :69500Training Loss: 146538.53246986866\n",
            "batch :70000Training Loss: 147620.35178768635\n",
            "batch :70500Training Loss: 148708.01376223564\n",
            "batch :71000Training Loss: 149797.9902881384\n",
            "batch :71500Training Loss: 150883.3107868433\n",
            "batch :72000Training Loss: 151970.06446290016\n",
            "batch :72500Training Loss: 153058.38012862206\n",
            "batch :73000Training Loss: 154159.12248063087\n",
            "batch :73500Training Loss: 155243.2977553606\n",
            "batch :74000Training Loss: 156330.80381131172\n",
            "batch :74500Training Loss: 157422.89836716652\n",
            "batch :75000Training Loss: 158534.2330545187\n",
            "batch :75500Training Loss: 159625.4761853218\n",
            "traing loss epoch1:0.15273361703281627\n",
            "time : 5:11:23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 2 ========\n",
            "batch :0Training Loss: 1.7710051536560059\n",
            "batch :500Training Loss: 1000.5923355817795\n",
            "batch :1000Training Loss: 1994.8049125671387\n",
            "batch :1500Training Loss: 3016.090177476406\n",
            "batch :2000Training Loss: 4016.832619905472\n",
            "batch :2500Training Loss: 5041.817929983139\n",
            "batch :3000Training Loss: 6059.141159415245\n",
            "batch :3500Training Loss: 7070.916650772095\n",
            "batch :4000Training Loss: 8093.582837104797\n",
            "batch :4500Training Loss: 9119.4923671484\n",
            "batch :5000Training Loss: 10142.546251773834\n",
            "batch :5500Training Loss: 11158.155492782593\n",
            "batch :6000Training Loss: 12163.843958437443\n",
            "batch :6500Training Loss: 13180.03725939989\n",
            "batch :7000Training Loss: 14198.459120512009\n",
            "batch :7500Training Loss: 15225.758615851402\n",
            "batch :8000Training Loss: 16245.111999809742\n",
            "batch :8500Training Loss: 17267.669735491276\n",
            "batch :9000Training Loss: 18285.899792850018\n",
            "batch :9500Training Loss: 19313.707645833492\n",
            "batch :10000Training Loss: 20336.421181499958\n",
            "batch :10500Training Loss: 21366.741517961025\n",
            "batch :11000Training Loss: 22395.79728883505\n",
            "batch :11500Training Loss: 23426.109500825405\n",
            "batch :12000Training Loss: 24477.314540088177\n",
            "batch :12500Training Loss: 25493.73469120264\n",
            "batch :13000Training Loss: 26521.15697747469\n",
            "batch :13500Training Loss: 27546.573072493076\n",
            "batch :14000Training Loss: 28582.883807361126\n",
            "batch :14500Training Loss: 29612.581175148487\n",
            "batch :15000Training Loss: 30635.112172663212\n",
            "batch :15500Training Loss: 31673.488952815533\n",
            "batch :16000Training Loss: 32703.368573367596\n",
            "batch :16500Training Loss: 33722.349747657776\n",
            "batch :17000Training Loss: 34755.60761535168\n",
            "batch :17500Training Loss: 35790.50496721268\n",
            "batch :18000Training Loss: 36817.423295259476\n",
            "batch :18500Training Loss: 37847.37929713726\n",
            "batch :19000Training Loss: 38885.15256369114\n",
            "batch :19500Training Loss: 39922.615381121635\n",
            "batch :20000Training Loss: 40944.881591796875\n",
            "batch :20500Training Loss: 41983.52291226387\n",
            "batch :21000Training Loss: 43011.61499559879\n",
            "batch :21500Training Loss: 44060.915387034416\n",
            "batch :22000Training Loss: 45092.96662247181\n",
            "batch :22500Training Loss: 46138.71953165531\n",
            "batch :23000Training Loss: 47172.57734978199\n",
            "batch :23500Training Loss: 48214.22720551491\n",
            "batch :24000Training Loss: 49243.424647688866\n",
            "batch :24500Training Loss: 50280.2621871233\n",
            "batch :25000Training Loss: 51318.57797026634\n",
            "batch :25500Training Loss: 52361.33782398701\n",
            "batch :26000Training Loss: 53408.9521433115\n",
            "batch :26500Training Loss: 54444.33403402567\n",
            "batch :27000Training Loss: 55491.08788996935\n",
            "batch :27500Training Loss: 56529.40921860933\n",
            "batch :28000Training Loss: 57569.44486659765\n",
            "batch :28500Training Loss: 58609.556663274765\n",
            "batch :29000Training Loss: 59645.440853476524\n",
            "batch :29500Training Loss: 60695.602984547615\n",
            "batch :30000Training Loss: 61737.8305362463\n",
            "batch :30500Training Loss: 62781.72782564163\n",
            "batch :31000Training Loss: 63827.49716114998\n",
            "batch :31500Training Loss: 64865.30187559128\n",
            "batch :32000Training Loss: 65905.8731637001\n",
            "batch :32500Training Loss: 66946.40194892883\n",
            "batch :33000Training Loss: 68001.27285420895\n",
            "batch :33500Training Loss: 69052.01071155071\n",
            "batch :34000Training Loss: 70109.54520380497\n",
            "batch :34500Training Loss: 71140.39075422287\n",
            "batch :35000Training Loss: 72185.78996253014\n",
            "batch :35500Training Loss: 73236.23684096336\n",
            "batch :36000Training Loss: 74274.92576867342\n",
            "batch :36500Training Loss: 75319.74381440878\n",
            "batch :37000Training Loss: 76380.68399208784\n",
            "batch :37500Training Loss: 77441.20094376802\n",
            "batch :38000Training Loss: 78482.2615762353\n",
            "batch :38500Training Loss: 79543.67050606012\n",
            "batch :39000Training Loss: 80609.20438724756\n",
            "batch :39500Training Loss: 81669.56185358763\n",
            "batch :40000Training Loss: 82735.4336925149\n",
            "batch :40500Training Loss: 83791.88643413782\n",
            "batch :41000Training Loss: 84854.99603420496\n",
            "batch :41500Training Loss: 85923.23600369692\n",
            "batch :42000Training Loss: 86997.2452327013\n",
            "batch :42500Training Loss: 88070.46456229687\n",
            "batch :43000Training Loss: 89146.71699023247\n",
            "batch :43500Training Loss: 90199.0936717987\n",
            "batch :44000Training Loss: 91275.73473626375\n",
            "batch :44500Training Loss: 92346.73896104097\n",
            "batch :45000Training Loss: 93408.54420870543\n",
            "batch :45500Training Loss: 94480.03801184893\n",
            "batch :46000Training Loss: 95559.13675028086\n",
            "batch :46500Training Loss: 96602.42071288824\n",
            "batch :47000Training Loss: 97684.54776579142\n",
            "batch :47500Training Loss: 98749.30494612455\n",
            "batch :48000Training Loss: 99828.4895452857\n",
            "batch :48500Training Loss: 100881.78949004412\n",
            "batch :49000Training Loss: 101964.01374453306\n",
            "batch :49500Training Loss: 103007.3792629838\n",
            "batch :50000Training Loss: 104069.59662139416\n",
            "batch :50500Training Loss: 105115.28276145458\n",
            "batch :51000Training Loss: 106189.1474827528\n",
            "batch :51500Training Loss: 107251.75649416447\n",
            "batch :52000Training Loss: 108312.5155287981\n",
            "batch :52500Training Loss: 109387.25451481342\n",
            "batch :53000Training Loss: 110450.50080537796\n",
            "batch :53500Training Loss: 111520.4582157135\n",
            "batch :54000Training Loss: 112582.57065951824\n",
            "batch :54500Training Loss: 113655.58310222626\n",
            "batch :55000Training Loss: 114730.17513942719\n",
            "batch :55500Training Loss: 115803.06756472588\n",
            "batch :56000Training Loss: 116870.50346398354\n",
            "batch :56500Training Loss: 117943.94340348244\n",
            "batch :57000Training Loss: 119011.84969383478\n",
            "batch :57500Training Loss: 120093.02402037382\n",
            "batch :58000Training Loss: 121171.7558786273\n",
            "batch :58500Training Loss: 122246.67801862955\n",
            "batch :59000Training Loss: 123321.30811661482\n",
            "batch :59500Training Loss: 124392.11557108164\n",
            "batch :60000Training Loss: 125466.24874705076\n",
            "batch :60500Training Loss: 126545.41189092398\n",
            "batch :61000Training Loss: 127635.83053201437\n",
            "batch :61500Training Loss: 128722.01552909613\n",
            "batch :62000Training Loss: 129808.69164448977\n",
            "batch :62500Training Loss: 130888.61495417356\n",
            "batch :63000Training Loss: 131964.36524313688\n",
            "batch :63500Training Loss: 133025.57998806238\n",
            "batch :64000Training Loss: 134121.39104813337\n",
            "batch :64500Training Loss: 135204.20979100466\n",
            "batch :65000Training Loss: 136286.70124667883\n",
            "batch :65500Training Loss: 137355.09083610773\n",
            "batch :66000Training Loss: 138455.59135621786\n",
            "batch :66500Training Loss: 139532.1410523057\n",
            "batch :67000Training Loss: 140615.21619182825\n",
            "batch :67500Training Loss: 141689.07138186693\n",
            "batch :68000Training Loss: 142774.57341855764\n",
            "batch :68500Training Loss: 143864.25994962454\n",
            "batch :69000Training Loss: 144940.51013845205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeYlDf1YeNDA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8c0f5b2-1e5f-4886-b2c1-c384464d8686"
      },
      "source": [
        "epochs = 5\n",
        "for epoc in range(epochs):\n",
        "  t0 = time.time()\n",
        "  print(\"\")\n",
        "  print('======== Epoch {} ========'.format(epoc+1))\n",
        "  model.train()\n",
        "  total_train_loss = 0\n",
        "  for i,batch in enumerate(train_dataset):\n",
        "    title = []\n",
        "    body = []\n",
        "    for item in batch['title'].numpy():\n",
        "      title.append(item.decode('utf-8'))\n",
        "    for item in batch['body'].numpy():\n",
        "      body.append(item.decode('utf-8')) \n",
        "    encoded_title = encode_file(tokenizer,title,120)\n",
        "    encoded_body = encode_file(tokenizer,body,512)\n",
        "    ids = torch.stack([x[\"input_ids\"] for x in encoded_body]).squeeze()# BS x SL\n",
        "    mask = torch.stack([x[\"attention_mask\"] for x in encoded_body]).squeeze() # BS x SL\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    y = torch.stack([x[\"input_ids\"] for x in encoded_title]).squeeze()\n",
        "    y = y.to(device, dtype = torch.long)\n",
        "    y_ids = y[:, :-1].contiguous()\n",
        "    lm_labels = y[:, 1:].clone().detach()\n",
        "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "    ids = ids.to(device, dtype = torch.long)\n",
        "    mask = mask.to(device, dtype = torch.long)\n",
        "    model.zero_grad()\n",
        "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "    loss = outputs[0]\n",
        "    total_train_loss += loss.item()\n",
        "    \n",
        "    if i%500 == 0:\n",
        "      print(\"batch :\" + str(i)+ \"Training Loss: \" +str(total_train_loss))\n",
        "    if i % 20000 == 0:\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
        "        torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "  training_time = format_time(time.time() - t0)\n",
        "  avg_train_loss = total_train_loss / 1050994\n",
        "  print(\"traing loss epoch\"+str(epoc+1)+\":\"+str(avg_train_loss))\n",
        "  print(\"time : {}\".format(training_time))\n",
        "  \n",
        "  model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "  model_to_save.save_pretrained(output_dir)\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "  torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
        "  torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 ========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py:1128: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch :0Training Loss: 2.553109884262085\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch :500Training Loss: 988.6108185052872\n",
            "batch :1000Training Loss: 2005.3290561437607\n",
            "batch :1500Training Loss: 3015.828029513359\n",
            "batch :2000Training Loss: 4015.1137213110924\n",
            "batch :2500Training Loss: 5047.26597648859\n",
            "batch :3000Training Loss: 6058.422338426113\n",
            "batch :3500Training Loss: 7077.438912451267\n",
            "batch :4000Training Loss: 8089.76018935442\n",
            "batch :4500Training Loss: 9113.382313072681\n",
            "batch :5000Training Loss: 10126.40777951479\n",
            "batch :5500Training Loss: 11147.421782910824\n",
            "batch :6000Training Loss: 12163.933211863041\n",
            "batch :6500Training Loss: 13192.830403149128\n",
            "batch :7000Training Loss: 14213.68697053194\n",
            "batch :7500Training Loss: 15222.291413486004\n",
            "batch :8000Training Loss: 16257.561263740063\n",
            "batch :8500Training Loss: 17281.995755016804\n",
            "batch :9000Training Loss: 18309.77070862055\n",
            "batch :9500Training Loss: 19328.594387590885\n",
            "batch :10000Training Loss: 20358.44654637575\n",
            "batch :10500Training Loss: 21383.82090872526\n",
            "batch :11000Training Loss: 22395.991100132465\n",
            "batch :11500Training Loss: 23436.579974532127\n",
            "batch :12000Training Loss: 24470.933621823788\n",
            "batch :12500Training Loss: 25502.786699593067\n",
            "batch :13000Training Loss: 26522.872064590454\n",
            "batch :13500Training Loss: 27547.289636731148\n",
            "batch :14000Training Loss: 28566.677658081055\n",
            "batch :14500Training Loss: 29598.545010864735\n",
            "batch :15000Training Loss: 30643.568377792835\n",
            "batch :15500Training Loss: 31659.10841012001\n",
            "batch :16000Training Loss: 32695.010676026344\n",
            "batch :16500Training Loss: 33729.74623000622\n",
            "batch :17000Training Loss: 34753.94160032272\n",
            "batch :17500Training Loss: 35784.896824002266\n",
            "batch :18000Training Loss: 36820.321964502335\n",
            "batch :18500Training Loss: 37850.85390460491\n",
            "batch :19000Training Loss: 38890.87042307854\n",
            "batch :19500Training Loss: 39931.545104026794\n",
            "batch :20000Training Loss: 40971.86596751213\n",
            "batch :20500Training Loss: 42011.51931488514\n",
            "batch :21000Training Loss: 43046.39331817627\n",
            "batch :21500Training Loss: 44075.81229168177\n",
            "batch :22000Training Loss: 45104.50653928518\n",
            "batch :22500Training Loss: 46145.09866946936\n",
            "batch :23000Training Loss: 47170.29158526659\n",
            "batch :23500Training Loss: 48213.50114685297\n",
            "batch :24000Training Loss: 49258.91857820749\n",
            "batch :24500Training Loss: 50294.34273546934\n",
            "batch :25000Training Loss: 51338.27565032244\n",
            "batch :25500Training Loss: 52389.16844922304\n",
            "batch :26000Training Loss: 53425.81936454773\n",
            "batch :26500Training Loss: 54486.74635183811\n",
            "batch :27000Training Loss: 55528.039887428284\n",
            "batch :27500Training Loss: 56569.26611506939\n",
            "batch :28000Training Loss: 57608.307594418526\n",
            "batch :28500Training Loss: 58646.868310689926\n",
            "batch :29000Training Loss: 59686.625453829765\n",
            "batch :29500Training Loss: 60728.21053481102\n",
            "batch :30000Training Loss: 61772.386016488075\n",
            "batch :30500Training Loss: 62791.94119155407\n",
            "batch :31000Training Loss: 63841.719725847244\n",
            "batch :31500Training Loss: 64882.14171457291\n",
            "batch :32000Training Loss: 65922.65756368637\n",
            "batch :32500Training Loss: 66964.91374742985\n",
            "batch :33000Training Loss: 68008.98452448845\n",
            "batch :33500Training Loss: 69064.33863252401\n",
            "batch :34000Training Loss: 70115.83705502748\n",
            "batch :34500Training Loss: 71160.63288122416\n",
            "batch :35000Training Loss: 72217.49175447226\n",
            "batch :35500Training Loss: 73255.90512442589\n",
            "batch :36000Training Loss: 74325.22755897045\n",
            "batch :36500Training Loss: 75376.39448696375\n",
            "batch :37000Training Loss: 76418.49297189713\n",
            "batch :37500Training Loss: 77476.5602388382\n",
            "batch :38000Training Loss: 78525.72156071663\n",
            "batch :38500Training Loss: 79579.35931062698\n",
            "batch :39000Training Loss: 80636.34726309776\n",
            "batch :39500Training Loss: 81711.10980796814\n",
            "batch :40000Training Loss: 82777.03361856937\n",
            "batch :40500Training Loss: 83839.82908922434\n",
            "batch :41000Training Loss: 84894.67470628023\n",
            "batch :41500Training Loss: 85943.911036551\n",
            "batch :42000Training Loss: 87001.29159480333\n",
            "batch :42500Training Loss: 88067.03864759207\n",
            "batch :43000Training Loss: 89124.30963659286\n",
            "batch :43500Training Loss: 90208.26752829552\n",
            "batch :44000Training Loss: 91274.86309075356\n",
            "batch :44500Training Loss: 92352.35849559307\n",
            "batch :45000Training Loss: 93419.45126569271\n",
            "batch :45500Training Loss: 94487.44221770763\n",
            "batch :46000Training Loss: 95563.22666943073\n",
            "batch :46500Training Loss: 96620.78816771507\n",
            "batch :47000Training Loss: 97678.40587210655\n",
            "batch :47500Training Loss: 98728.32748603821\n",
            "batch :48000Training Loss: 99799.27349376678\n",
            "batch :48500Training Loss: 100869.79598236084\n",
            "batch :49000Training Loss: 101933.76145625114\n",
            "batch :49500Training Loss: 102979.95181584358\n",
            "batch :50000Training Loss: 104035.38727790117\n",
            "batch :50500Training Loss: 105095.03979843855\n",
            "batch :51000Training Loss: 106158.86821597815\n",
            "batch :51500Training Loss: 107234.42446237803\n",
            "batch :52000Training Loss: 108328.81918984652\n",
            "batch :52500Training Loss: 109381.34311109781\n",
            "batch :53000Training Loss: 110439.03029054403\n",
            "batch :53500Training Loss: 111496.33642375469\n",
            "batch :54000Training Loss: 112568.24502468109\n",
            "batch :54500Training Loss: 113638.04394698143\n",
            "batch :55000Training Loss: 114698.92293047905\n",
            "batch :55500Training Loss: 115792.64474081993\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}